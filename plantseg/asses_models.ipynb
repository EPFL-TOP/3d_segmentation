{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stardist.matching import matching\n",
    "from functions import my_combinations_,filter_combinations_,from_dict_to_folder_path,divide_combi,dict_str,get_image_path,from_dict_to_folder_path,base_dir\n",
    "import os\n",
    "import pandas as pd\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "\n",
    "result_path=os.path.join(base_dir+'/results.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# done computations: 1996\n",
      "# all computations: 3718\n",
      "# computations to do: 1722\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def safe_convert(val):\n",
    "    \"\"\"Safely convert a value from CSV to its intended type.\n",
    "    Args:\n",
    "        val: The value to convert, which may be a string representation of a tuple or a raw value.\n",
    "    Returns:\n",
    "        The converted value, or None if the value is NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    if isinstance(val, str) and val.startswith('(') and val.endswith(')'):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return val\n",
    "    return val\n",
    "\n",
    "done_computations_from_csv=(pd.read_csv(result_path).iloc[:,:-2].map(safe_convert)).to_dict('records')  \n",
    "combis=my_combinations_()\n",
    "\n",
    "print('# done computations:',len(done_computations_from_csv))\n",
    "print('# all computations:',len(combis))\n",
    "results_to_be_computed=filter_combinations_(new_combinations=combis,done_combinations=done_computations_from_csv)\n",
    "print('# computations to do:',len(results_to_be_computed))\n",
    "\n",
    "# computations to do = all combinations - done computations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses the segemented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function matching in module stardist.matching:\n",
      "\n",
      "matching(y_true, y_pred, thresh=0.5, criterion='iou', report_matches=False)\n",
      "    Calculate detection/instance segmentation metrics between ground truth and predicted label images.\n",
      "\n",
      "    Currently, the following metrics are implemented:\n",
      "\n",
      "    'fp', 'tp', 'fn', 'precision', 'recall', 'accuracy', 'f1', 'criterion', 'thresh', 'n_true', 'n_pred', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'\n",
      "\n",
      "    Corresponding objects of y_true and y_pred are counted as true positives (tp), false positives (fp), and false negatives (fn)\n",
      "    whether their intersection over union (IoU) >= thresh (for criterion='iou', which can be changed)\n",
      "\n",
      "    * mean_matched_score is the mean IoUs of matched true positives\n",
      "\n",
      "    * mean_true_score is the mean IoUs of matched true positives but normalized by the total number of GT objects\n",
      "\n",
      "    * panoptic_quality defined as in Eq. 1 of Kirillov et al. \"Panoptic Segmentation\", CVPR 2019\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true: ndarray\n",
      "        ground truth label image (integer valued)\n",
      "    y_pred: ndarray\n",
      "        predicted label image (integer valued)\n",
      "    thresh: float\n",
      "        threshold for matching criterion (default 0.5)\n",
      "    criterion: string\n",
      "        matching criterion (default IoU)\n",
      "    report_matches: bool\n",
      "        if True, additionally calculate matched_pairs and matched_scores (note, that this returns even gt-pred pairs whose scores are below  'thresh')\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Matching object with different metrics as attributes\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> y_true = np.zeros((100,100), np.uint16)\n",
      "    >>> y_true[10:20,10:20] = 1\n",
      "    >>> y_pred = np.roll(y_true,5,axis = 0)\n",
      "\n",
      "    >>> stats = matching(y_true, y_pred)\n",
      "    >>> print(stats)\n",
      "    Matching(criterion='iou', thresh=0.5, fp=1, tp=0, fn=1, precision=0, recall=0, accuracy=0, f1=0, n_true=1, n_pred=1, mean_true_score=0.0, mean_matched_score=0.0, panoptic_quality=0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(matching)\n",
    "\n",
    "'''\n",
    "The documentation for the StarDist library for the matching can be found at:\n",
    "https://github.com/stardist/stardist/blob/main/stardist/matching.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 500/1722 [11:05<28:40,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error was :\n",
      "[Errno 2] No such file or directory: '/mnt/e/PROJECTS-01/Adrian/Bachelor_project/GridSearch/Mouse_model/_Threshold_0.4_SigmaSeed_0.1_SigmaWeight_0.5_MinSize_100.0_Alpha_0.8_PixelPitch_None_Mode_multicut/_Beta_0.7_MinSize2_100.0/_'\n",
      "Combi was : {'Threshold': 0.4, 'SigmaSeed': 0.1, 'SigmaWeight': 0.5, 'MinSize': 100.0, 'Alpha': 0.8, 'PixelPitch': None, 'Mode': 'multicut', 'Beta': 0.7, 'MinSize2': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1722/1722 [39:05<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "# y_true is the annotated ground truth image\n",
    "y_true = tifffile.imread('/mnt/e/PROJECTS-01/Adrian/Bachelor_project/annotated_data/pos4_t0001_Channel1_annotated_30cells.tif')[77:195,437:1446,654:]\n",
    "# y_true2 is the cellpose segmented image\n",
    "y_true2 = tifffile.imread('/mnt/e/PROJECTS-01/Adrian/Bachelor_project/processed_data/Cellpose/no_gauss/t0001_bi1_cellpose_3d_cyto_segmentation_Feyza_cp_masks.tif')\n",
    "\n",
    "def process_one_combi(combi):\n",
    "    \"\"\"Process a single combination and asses the segmentation results.\n",
    "    Args:\n",
    "        combi: A dictionary representing the combination to process.\n",
    "    Returns:\n",
    "        A dictionary with the results of the processing, including mean matched score and accuracy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        divided_combis=[from_dict_to_folder_path(dict_str(combo),path=\"\") for combo in divide_combi(combi)] \n",
    "        path_folder=base_dir+\"/\"+os.path.join(divided_combis[0],divided_combis[1],divided_combis[2])\n",
    "        condition=lambda file_name:(file_name.endswith('.tiff') or file_name.endswith('.tif'))\n",
    "        path_file=get_image_path(path_folder,condition)\n",
    "        if os.path.basename(path_file).startswith(\"WRONG\"):\n",
    "            mean_matched_score=0\n",
    "            accuracy=0\n",
    "        else:\n",
    "            y_pred=tifffile.imread(path_file)\n",
    "            mean_matched_score= float((matching(y_true, y_pred)._asdict())[\"mean_matched_score\"])\n",
    "            accuracy=float((matching(y_true2,y_pred)._asdict())[\"accuracy\"])\n",
    "        result_entry = combi.copy() \n",
    "        result_entry[\"Mean Matched Score\"] = mean_matched_score \n",
    "        result_entry[\"accuracy\"]=accuracy\n",
    "        return result_entry\n",
    "    except Exception as e:\n",
    "        print(\"Error was :\")\n",
    "        print(e)\n",
    "        print(\"Combi was :\", combi)\n",
    "        return None\n",
    "\n",
    "# Process all combinations in parallel\n",
    "results = Parallel(n_jobs=20)(delayed(process_one_combi)(c) for c in tqdm(results_to_be_computed))\n",
    "results = [r for r in results if r is not None]\n",
    "\n",
    "# Save all at once\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(result_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Threshold  SigmaSeed  SigmaWeight  MinSize  Alpha     PixelPitch  Mode  \\\n",
      "755        0.4        0.2          0.0     90.0    0.7  (1.441, 1, 1)  gasp   \n",
      "754        0.4        0.2          0.0     90.0    0.7  (1.441, 1, 1)  gasp   \n",
      "745        0.4        0.2          0.0     90.0    0.7  (1.441, 1, 1)  gasp   \n",
      "744        0.4        0.2          0.0     90.0    0.7  (1.441, 1, 1)  gasp   \n",
      "656        0.4        0.2          0.0    100.0    0.7  (1.441, 1, 1)  gasp   \n",
      "\n",
      "     Beta  MinSize2  Treshold2  Instances  Mean Matched Score  accuracy  \n",
      "755   0.7      75.0        0.2       True             0.78103  0.070785  \n",
      "754   0.7      75.0        0.2      False             0.78103  0.070798  \n",
      "745   0.7     100.0        0.1       True             0.78103  0.074730  \n",
      "744   0.7     100.0        0.1      False             0.78103  0.074743  \n",
      "656   0.7      75.0        0.1      False             0.78103  0.074579  \n",
      "      Threshold  SigmaSeed  SigmaWeight  MinSize  Alpha     PixelPitch  \\\n",
      "1991        0.7        0.2          0.0    100.0   1.25  (1.441, 1, 1)   \n",
      "1990        0.7        0.2          0.0    100.0   1.25  (1.441, 1, 1)   \n",
      "1994        0.7        0.2          0.0    100.0   1.25            NaN   \n",
      "1995        0.7        0.2          0.0    100.0   1.25            NaN   \n",
      "1668        0.5        0.2          0.0    100.0   1.25            NaN   \n",
      "\n",
      "          Mode  Beta  MinSize2  Treshold2  Instances  Mean Matched Score  \\\n",
      "1991      gasp   1.0      75.0        0.1      False            0.722506   \n",
      "1990      gasp   1.0     100.0        0.1      False            0.722506   \n",
      "1994      gasp   1.0     100.0        0.1      False            0.719794   \n",
      "1995      gasp   1.0      75.0        0.1      False            0.719794   \n",
      "1668  multicut   1.0     100.0        0.1      False            0.726228   \n",
      "\n",
      "      accuracy  \n",
      "1991  0.292969  \n",
      "1990  0.292969  \n",
      "1994  0.290665  \n",
      "1995  0.290665  \n",
      "1668  0.288613  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sort and print the top 5 combinations by 'Mean Matched Score' descending\n",
    "df = pd.read_csv(result_path)\n",
    "df_sorted = df.sort_values(by='Mean Matched Score', ascending=False)\n",
    "top_5 = df_sorted.head(5)\n",
    "print(top_5)\n",
    "\n",
    "# Sort and print the top 5 combinations by 'accuracy' descending\n",
    "df_sorted_2 = df.sort_values(by='accuracy', ascending=False)\n",
    "top_5_2 = df_sorted_2.head(5)\n",
    "print(top_5_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures \n",
    "Created with https://claude.ai/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "class ParameterAnalyzer:\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"Initialize the analyzer with data from CSV file\"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Clean and prepare the data for analysis\"\"\"\n",
    "        # Convert boolean columns\n",
    "        bool_cols = ['Instances']\n",
    "        for col in bool_cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].map({'TRUE': True, 'FALSE': False})\n",
    "        \n",
    "        # Identify parameter columns (exclude metrics)\n",
    "        self.metric_cols = ['Mean Matched Score', 'accuracy']\n",
    "        self.param_cols = [col for col in self.df.columns \n",
    "                          if col not in self.metric_cols and col != 'PixelPitch']\n",
    "        \n",
    "        # Create filtered dataset (non-zero values)\n",
    "        self.df_filtered = self.df[\n",
    "            (self.df['Mean Matched Score'] > 0) | (self.df['accuracy'] > 0)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Total combinations: {len(self.df)}\")\n",
    "        print(f\"Non-zero combinations: {len(self.df_filtered)}\")\n",
    "        print(f\"Success rate: {len(self.df_filtered)/len(self.df)*100:.1f}%\")\n",
    "    \n",
    "    def plot_correlation_heatmap(self, use_filtered=True):\n",
    "        \"\"\"Create correlation heatmap of all numeric parameters\"\"\"\n",
    "        data = self.df_filtered if use_filtered else self.df\n",
    "        \n",
    "        # Select only numeric columns for correlation\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = data[numeric_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "                   center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "        \n",
    "        plt.title('Parameter Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_parameter_heatmap(self, x_param, y_param, metric='Mean Matched Score', \n",
    "                              use_filtered=True, aggregation='mean'):\n",
    "        \"\"\"Create a heatmap for two parameters colored by a metric\"\"\"\n",
    "        data = self.df_filtered if use_filtered else self.df\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            print(\"No data to plot after filtering\")\n",
    "            return\n",
    "        \n",
    "        # Create pivot table\n",
    "        if aggregation == 'mean':\n",
    "            pivot_data = data.pivot_table(values=metric, index=y_param, \n",
    "                                        columns=x_param, aggfunc='mean')\n",
    "        elif aggregation == 'max':\n",
    "            pivot_data = data.pivot_table(values=metric, index=y_param, \n",
    "                                        columns=x_param, aggfunc='max')\n",
    "        else:  # count\n",
    "            pivot_data = data.pivot_table(values=metric, index=y_param, \n",
    "                                        columns=x_param, aggfunc='count')\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                   cbar_kws={'label': metric}, linewidths=0.5)\n",
    "        \n",
    "        plt.title(f'{metric} by {x_param} vs {y_param} '\n",
    "                 f'({aggregation} aggregation)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(x_param, fontsize=12)\n",
    "        plt.ylabel(y_param, fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_parameter_distribution(self, use_filtered=True):\n",
    "        \"\"\"Plot distribution of all parameters\"\"\"\n",
    "        data = self.df_filtered if use_filtered else self.df\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, param in enumerate(self.param_cols[:9]):  # Plot first 9 parameters\n",
    "            if i < len(axes):\n",
    "                if data[param].dtype in ['object', 'bool']:\n",
    "                    # For categorical data\n",
    "                    value_counts = data[param].value_counts()\n",
    "                    axes[i].bar(range(len(value_counts)), value_counts.values)\n",
    "                    axes[i].set_xticks(range(len(value_counts)))\n",
    "                    axes[i].set_xticklabels(value_counts.index, rotation=45)\n",
    "                else:\n",
    "                    # For numeric data\n",
    "                    axes[i].hist(data[param], bins=20, alpha=0.7, edgecolor='black')\n",
    "                \n",
    "                axes[i].set_title(param, fontweight='bold')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(self.param_cols), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Parameter Distributions', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_performance_by_parameter(self, param, metric='Mean Matched Score', use_filtered=True):\n",
    "        \"\"\"Plot performance metric by a specific parameter\"\"\"\n",
    "        data = self.df_filtered if use_filtered else self.df\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        if data[param].dtype in ['object', 'bool'] or data[param].nunique() < 10:\n",
    "            # Box plot for categorical or few unique values\n",
    "            sns.boxplot(data=data, x=param, y=metric)\n",
    "            plt.xticks(rotation=45)\n",
    "        else:\n",
    "            # Scatter plot for continuous variables\n",
    "            plt.scatter(data[param], data[metric], alpha=0.6)\n",
    "            # Add trend line\n",
    "            z = np.polyfit(data[param], data[metric], 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(data[param], p(data[param]), \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.title(f'{metric} by {param}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(param, fontsize=12)\n",
    "        plt.ylabel(metric, fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_top_combinations(self, metric='Mean Matched Score', top_n=10):\n",
    "        \"\"\"Plot the top N combinations\"\"\"\n",
    "        top_combos = self.df_filtered.nlargest(top_n, metric)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Bar plot of top combinations\n",
    "        combo_labels = [f\"Combo {i+1}\" for i in range(len(top_combos))]\n",
    "        ax1.bar(combo_labels, top_combos[metric], color='skyblue', edgecolor='navy')\n",
    "        ax1.set_title(f'Top {top_n} Combinations by {metric}', fontweight='bold')\n",
    "        ax1.set_ylabel(metric)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Heatmap of parameter values for top combinations\n",
    "        param_subset = ['Threshold', 'Beta', 'Alpha', 'MinSize', 'MinSize2']  # Key parameters\n",
    "        available_params = [p for p in param_subset if p in top_combos.columns]\n",
    "        \n",
    "        if available_params:\n",
    "            heatmap_data = top_combos[available_params].T\n",
    "            sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='viridis', \n",
    "                       ax=ax2, cbar_kws={'label': 'Parameter Value'})\n",
    "            ax2.set_title('Parameter Values for Top Combinations', fontweight='bold')\n",
    "            ax2.set_xlabel('Combination Rank')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return top_combos\n",
    "    \n",
    "    def plot_metric_comparison(self):\n",
    "        \"\"\"Compare the two metrics against each other\"\"\"\n",
    "        if len(self.df_filtered) == 0:\n",
    "            print(\"No filtered data available for comparison\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Scatter plot\n",
    "        plt.scatter(self.df_filtered['Mean Matched Score'], \n",
    "                   self.df_filtered['accuracy'], alpha=0.6, s=50)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        corr = self.df_filtered['Mean Matched Score'].corr(self.df_filtered['accuracy'])\n",
    "        plt.title(f'Mean Matched Score vs Accuracy\\n(Correlation: {corr:.3f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.xlabel('Mean Matched Score', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(self.df_filtered) > 1:\n",
    "            z = np.polyfit(self.df_filtered['Mean Matched Score'], \n",
    "                          self.df_filtered['accuracy'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_trend = np.linspace(self.df_filtered['Mean Matched Score'].min(),\n",
    "                                 self.df_filtered['Mean Matched Score'].max(), 100)\n",
    "            plt.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def comprehensive_analysis(self):\n",
    "        \"\"\"Run a comprehensive analysis with multiple visualizations\"\"\"\n",
    "        print(\"=== COMPREHENSIVE PARAMETER ANALYSIS ===\\n\")\n",
    "        \n",
    "        # 1. Basic statistics\n",
    "        print(\"1. BASIC STATISTICS:\")\n",
    "        print(f\"Best Mean Matched Score: {self.df['Mean Matched Score'].max():.6f}\")\n",
    "        print(f\"Best Accuracy: {self.df['accuracy'].max():.6f}\")\n",
    "        print(f\"Average Mean Matched Score (non-zero): {self.df_filtered['Mean Matched Score'].mean():.6f}\")\n",
    "        print(f\"Average Accuracy (non-zero): {self.df_filtered['accuracy'].mean():.6f}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # 2. Correlation heatmap\n",
    "        print(\"2. PLOTTING: Correlation Matrix\")\n",
    "        self.plot_correlation_heatmap()\n",
    "        \n",
    "        # 3. Parameter distributions\n",
    "        print(\"3. PLOTTING: Parameter Distributions\")\n",
    "        self.plot_parameter_distribution()\n",
    "        \n",
    "        # 4. Key parameter heatmaps\n",
    "        print(\"4. PLOTTING: Key Parameter Heatmaps\")\n",
    "        key_combinations = [\n",
    "            ('Threshold', 'Beta'),\n",
    "            ('Alpha', 'Beta'),\n",
    "            ('MinSize', 'MinSize2'),\n",
    "            ('Threshold', 'Alpha')\n",
    "        ]\n",
    "        \n",
    "        for x_param, y_param in key_combinations:\n",
    "            if x_param in self.param_cols and y_param in self.param_cols:\n",
    "                print(f\"   - {x_param} vs {y_param}\")\n",
    "                self.plot_parameter_heatmap(x_param, y_param, 'Mean Matched Score')\n",
    "        \n",
    "        # 5. Performance by key parameters\n",
    "        print(\"5. PLOTTING: Performance by Key Parameters\")\n",
    "        key_params = ['Threshold', 'Beta', 'Alpha', 'MinSize']\n",
    "        for param in key_params:\n",
    "            if param in self.param_cols:\n",
    "                print(f\"   - Performance by {param}\")\n",
    "                self.plot_performance_by_parameter(param, 'Mean Matched Score')\n",
    "        \n",
    "        # 6. Top combinations\n",
    "        print(\"6. PLOTTING: Top Combinations\")\n",
    "        top_combos = self.plot_top_combinations('Mean Matched Score', 15)\n",
    "        \n",
    "        # 7. Metric comparison\n",
    "        print(\"7. PLOTTING: Metric Comparison\")\n",
    "        self.plot_metric_comparison()\n",
    "        \n",
    "        # 8. Best combination details\n",
    "        print(\"8. BEST COMBINATION DETAILS:\")\n",
    "        best_combo = self.df.loc[self.df['Mean Matched Score'].idxmax()]\n",
    "        print(\"\\nBest Mean Matched Score combination:\")\n",
    "        for param in self.param_cols:\n",
    "            if param in best_combo:\n",
    "                print(f\"  {param}: {best_combo[param]}\")\n",
    "        print(f\"  Mean Matched Score: {best_combo['Mean Matched Score']:.6f}\")\n",
    "        print(f\"  Accuracy: {best_combo['accuracy']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the analyzer with the path to the CSV file\n",
    "analyzer = ParameterAnalyzer(\"results.csv\")\n",
    "\n",
    "#Choose what you want to do\n",
    "\n",
    "\n",
    "analyzer.comprehensive_analysis()\n",
    "\n",
    "# Individual plots\n",
    "analyzer.plot_correlation_heatmap()\n",
    "analyzer.plot_parameter_heatmap('Alpha', 'Beta', 'Mean Matched Score')\n",
    "analyzer.plot_parameter_heatmap('Alpha', 'Beta', 'accuracy')\n",
    "analyzer.plot_performance_by_parameter('Alpha', 'Mean Matched Score')\n",
    "analyzer.plot_performance_by_parameter('Beta', 'Mean Matched Score')\n",
    "analyzer.plot_top_combinations('Mean Matched Score', 15)\n",
    "analyzer.plot_top_combinations('accuracy', 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StardDist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
